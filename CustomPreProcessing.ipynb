{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pyspark.ml.pipeline import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.ml.util import keyword_only\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class NLTKWordPunctTokenizer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super(NLTKWordPunctTokenizer, self).__init__()\n",
    "        self.stopwords = Param(self, \"stopwords\", \"\")\n",
    "        self._setDefault(stopwords=set())\n",
    "        kwargs = self.__init__._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        kwargs = self.setParams._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setStopwords(self, value):\n",
    "        self._paramMap[self.stopwords] = value\n",
    "        return self\n",
    "\n",
    "    def getStopwords(self):\n",
    "        return self.getOrDefault(self.stopwords)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        stopwords = self.getStopwords()\n",
    "\n",
    "        def f(s):\n",
    "            tokens = nltk.tokenize.wordpunct_tokenize(s)\n",
    "            return [t for t in tokens if t.lower() not in stopwords]\n",
    "\n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x104a7c0d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Hi I heard about ...|  [Hi, heard, Spark]|\n",
      "|    0|I wish Java could...|[wish, Java, coul...|\n",
      "|    1|Logistic regressi...|[Logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame = sqlContext.createDataFrame([\n",
    "  (0, \"Hi I heard about Spark\"),\n",
    "  (0, \"I wish Java could use case classes\"),\n",
    "  (1, \"Logistic regression models are neat\")\n",
    "], ['label', 'sentence'])\n",
    "\n",
    "tokenizer = NLTKWordPunctTokenizer(\n",
    "    inputCol=\"sentence\", outputCol=\"words\",  \n",
    "    stopwords=set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "tokenizer.transform(sentenceDataFrame).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "import loadFilesPartial as lfp\n",
    "data,Y=lfp.loadLabeled(\"./data/train\",10)\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeledData = zip(data,[y.item() for y in Y])\n",
    "labeledDataRdd = sc.parallelize(labeledData)\n",
    "df = sqlContext.createDataFrame(labeledDataRdd, ['review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              review|label|\n",
      "+--------------------+-----+\n",
      "|Bromwell High is ...|  1.0|\n",
      "|Homelessness (or ...|  1.0|\n",
      "|Brilliant over-ac...|  1.0|\n",
      "|This is easily th...|  1.0|\n",
      "|This is not the t...|  1.0|\n",
      "|This isn't the co...|  1.0|\n",
      "|Yes its an art......|  1.0|\n",
      "|In this \"critical...|  1.0|\n",
      "|THE NIGHT LISTENE...|  1.0|\n",
      "|You know, Robin W...|  1.0|\n",
      "|Story of a man wh...|  0.0|\n",
      "|Airport '77 start...|  0.0|\n",
      "|This film lacked ...|  0.0|\n",
      "|Sorry everyone,,,...|  0.0|\n",
      "|When I was little...|  0.0|\n",
      "|\"It appears that ...|  0.0|\n",
      "|The second attemp...|  0.0|\n",
      "|I don't know who ...|  0.0|\n",
      "|This film is medi...|  0.0|\n",
      "|The film is bad. ...|  0.0|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = NLTKWordPunctTokenizer(\n",
    "    inputCol=\"review\", outputCol=\"words\",  \n",
    "    stopwords=set(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|              review|label|               words|\n",
      "+--------------------+-----+--------------------+\n",
      "|Bromwell High is ...|  1.0|[Bromwell, High, ...|\n",
      "|Homelessness (or ...|  1.0|[Homelessness, (,...|\n",
      "|Brilliant over-ac...|  1.0|[Brilliant, -, ac...|\n",
      "|This is easily th...|  1.0|[easily, underrat...|\n",
      "|This is not the t...|  1.0|[typical, Mel, Br...|\n",
      "|This isn't the co...|  1.0|[isn, ', comedic,...|\n",
      "|Yes its an art......|  1.0|[Yes, art, ..., s...|\n",
      "|In this \"critical...|  1.0|[\", critically, a...|\n",
      "|THE NIGHT LISTENE...|  1.0|[NIGHT, LISTENER,...|\n",
      "|You know, Robin W...|  1.0|[know, ,, Robin, ...|\n",
      "|Story of a man wh...|  0.0|[Story, man, unna...|\n",
      "|Airport '77 start...|  0.0|[Airport, ', 77, ...|\n",
      "|This film lacked ...|  0.0|[film, lacked, so...|\n",
      "|Sorry everyone,,,...|  0.0|[Sorry, everyone,...|\n",
      "|When I was little...|  0.0|[little, parents,...|\n",
      "|\"It appears that ...|  0.0|[\", appears, many...|\n",
      "|The second attemp...|  0.0|[second, attempt,...|\n",
      "|I don't know who ...|  0.0|[', know, blame, ...|\n",
      "|This film is medi...|  0.0|[film, mediocre, ...|\n",
      "|The film is bad. ...|  0.0|[film, bad, ., wa...|\n",
      "+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
