{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Chargement d'une petite base de textes\n",
    "import loadFilesPartial as lfp\n",
    "data,Y=lfp.loadLabeled(\"./data/train\",10)\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Le item est nécessaire pour convertir au format Double (python natif) \n",
    "# car les dataframes semblent avoir des problèmes avec les types numpy\n",
    "labeledData = zip(data,[y.item() for y in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeledRdd = sc.parallelize(labeledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Définition d'une fonction de nettoyage : on remplace les balises par des espaces\n",
    "# à l'aide du parser de hmtl, et on met tout en minuscules.\n",
    "def cleanLower(doc):\n",
    "    clean = bfs(doc).get_text(separator=' ')\n",
    "    return clean.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On applique le nettoyage à la RDD\n",
    "cleanRdd = labeledRdd.map(lambda doc : (cleanLower(doc[0]),doc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On va utiliser la librairie spark ML donc on travaille avec des dataframes\n",
    "# EN THEORIE, c'est plus facile. C'est un mensonge.\n",
    "df = sqlContext.createDataFrame(cleanRdd, ['review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTrain, dfTest = df.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              review|label|\n",
      "+--------------------+-----+\n",
      "|homelessness (or ...|  1.0|\n",
      "|brilliant over-ac...|  1.0|\n",
      "|this is easily th...|  1.0|\n",
      "|this is not the t...|  1.0|\n",
      "|in this \"critical...|  1.0|\n",
      "|the night listene...|  1.0|\n",
      "|you know, robin w...|  1.0|\n",
      "|airport '77 start...|  0.0|\n",
      "|this film lacked ...|  0.0|\n",
      "|sorry everyone,,,...|  0.0|\n",
      "|the second attemp...|  0.0|\n",
      "|i don't know who ...|  0.0|\n",
      "|this film is medi...|  0.0|\n",
      "|the film is bad. ...|  0.0|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              review|label|\n",
      "+--------------------+-----+\n",
      "|bromwell high is ...|  1.0|\n",
      "|this isn't the co...|  1.0|\n",
      "|yes its an art......|  1.0|\n",
      "|story of a man wh...|  0.0|\n",
      "|when i was little...|  0.0|\n",
      "|\"it appears that ...|  0.0|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol='review', outputCol='words')\n",
    "dfTrainTok = tokenizer.transform(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=[u'homelessness', u'(or', u'houselessness', u'as', u'george', u'carlin', u'stated)', u'has', u'been', u'an', u'issue', u'for', u'years', u'but', u'never', u'a', u'plan', u'to', u'help', u'those', u'on', u'the', u'street', u'that', u'were', u'once', u'considered', u'human', u'who', u'did', u'everything', u'from', u'going', u'to', u'school,', u'work,', u'or', u'vote', u'for', u'the', u'matter.', u'most', u'people', u'think', u'of', u'the', u'homeless', u'as', u'just', u'a', u'lost', u'cause', u'while', u'worrying', u'about', u'things', u'such', u'as', u'racism,', u'the', u'war', u'on', u'iraq,', u'pressuring', u'kids', u'to', u'succeed,', u'technology,', u'the', u'elections,', u'inflation,', u'or', u'worrying', u'if', u\"they'll\", u'be', u'next', u'to', u'end', u'up', u'on', u'the', u'streets.', u'but', u'what', u'if', u'you', u'were', u'given', u'a', u'bet', u'to', u'live', u'on', u'the', u'streets', u'for', u'a', u'month', u'without', u'the', u'luxuries', u'you', u'once', u'had', u'from', u'a', u'home,', u'the', u'entertainment', u'sets,', u'a', u'bathroom,', u'pictures', u'on', u'the', u'wall,', u'a', u'computer,', u'and', u'everything', u'you', u'once', u'treasure', u'to', u'see', u'what', u\"it's\", u'like', u'to', u'be', u'homeless?', u'that', u'is', u'goddard', u\"bolt's\", u'lesson.', u'mel', u'brooks', u'(who', u'directs)', u'who', u'stars', u'as', u'bolt', u'plays', u'a', u'rich', u'man', u'who', u'has', u'everything', u'in', u'the', u'world', u'until', u'deciding', u'to', u'make', u'a', u'bet', u'with', u'a', u'sissy', u'rival', u'(jeffery', u'tambor)', u'to', u'see', u'if', u'he', u'can', u'live', u'in', u'the', u'streets', u'for', u'thirty', u'days', u'without', u'the', u'luxuries;', u'if', u'bolt', u'succeeds,', u'he', u'can', u'do', u'what', u'he', u'wants', u'with', u'a', u'future', u'project', u'of', u'making', u'more', u'buildings.', u'the', u\"bet's\", u'on', u'where', u'bolt', u'is', u'thrown', u'on', u'the', u'street', u'with', u'a', u'bracelet', u'on', u'his', u'leg', u'to', u'monitor', u'his', u'every', u'move', u'where', u'he', u\"can't\", u'step', u'off', u'the', u'sidewalk.', u\"he's\", u'given', u'the', u'nickname', u'pepto', u'by', u'a', u'vagrant', u'after', u\"it's\", u'written', u'on', u'his', u'forehead', u'where', u'bolt', u'meets', u'other', u'characters', u'including', u'a', u'woman', u'by', u'the', u'name', u'of', u'molly', u'(lesley', u'ann', u'warren)', u'an', u'ex-dancer', u'who', u'got', u'divorce', u'before', u'losing', u'her', u'home,', u'and', u'her', u'pals', u'sailor', u'(howard', u'morris)', u'and', u'fumes', u'(teddy', u'wilson)', u'who', u'are', u'already', u'used', u'to', u'the', u'streets.', u\"they're\", u'survivors.', u'bolt', u\"isn't.\", u\"he's\", u'not', u'used', u'to', u'reaching', u'mutual', u'agreements', u'like', u'he', u'once', u'did', u'when', u'being', u'rich', u'where', u\"it's\", u'fight', u'or', u'flight,', u'kill', u'or', u'be', u'killed.', u'while', u'the', u'love', u'connection', u'between', u'molly', u'and', u'bolt', u\"wasn't\", u'necessary', u'to', u'plot,', u'i', u'found', u'\"life', u'stinks\"', u'to', u'be', u'one', u'of', u'mel', u\"brooks'\", u'observant', u'films', u'where', u'prior', u'to', u'being', u'a', u'comedy,', u'it', u'shows', u'a', u'tender', u'side', u'compared', u'to', u'his', u'slapstick', u'work', u'such', u'as', u'blazing', u'saddles,', u'young', u'frankenstein,', u'or', u'spaceballs', u'for', u'the', u'matter,', u'to', u'show', u'what', u\"it's\", u'like', u'having', u'something', u'valuable', u'before', u'losing', u'it', u'the', u'next', u'day', u'or', u'on', u'the', u'other', u'hand', u'making', u'a', u'stupid', u'bet', u'like', u'all', u'rich', u'people', u'do', u'when', u'they', u\"don't\", u'know', u'what', u'to', u'do', u'with', u'their', u'money.', u'maybe', u'they', u'should', u'give', u'it', u'to', u'the', u'homeless', u'instead', u'of', u'using', u'it', u'like', u'monopoly', u'money.', u'or', u'maybe', u'this', u'film', u'will', u'inspire', u'you', u'to', u'help', u'others.'])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La ponctuation n'a pas l'air gérée très correctement...\n",
    "dfTrainTok.select('words').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# J'ai écrit cette fonction au cas où on souhaiterait absolument passer par des \n",
    "# pipelines, et intégrer les stopword dans la structure d'un transformer devait \n",
    "# passer par ce code grosso modo. Au final, on se passera surement des pipelines\n",
    "# car leurs possibilités sont limitées sur spark ml. Très limitées. C'est de la \n",
    "# merde en fait. Voila.\n",
    "\n",
    "import nltk\n",
    "from pyspark.ml.pipeline import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.ml.util import keyword_only\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class NLTKWordPunctTokenizer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super(NLTKWordPunctTokenizer, self).__init__()\n",
    "        self.stopwords = Param(self, \"stopwords\", \"\")\n",
    "        self._setDefault(stopwords=set())\n",
    "        kwargs = self.__init__._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        kwargs = self.setParams._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setStopwords(self, value):\n",
    "        self._paramMap[self.stopwords] = value\n",
    "        return self\n",
    "\n",
    "    def getStopwords(self):\n",
    "        return self.getOrDefault(self.stopwords)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        stopwords = self.getStopwords()\n",
    "\n",
    "        def f(s):\n",
    "            tokens = nltk.tokenize.wordpunct_tokenize(s)\n",
    "            return [t for t in tokens if t.lower() not in stopwords]\n",
    "\n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Par contre cette fonction marche bien\n",
    "tokenizerNoSw = NLTKWordPunctTokenizer(\n",
    "    inputCol=\"review\", outputCol=\"wordsNoSw\",  \n",
    "    stopwords=set(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTrainTokNoSw = tokenizerNoSw.transform(dfTrainTok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|              review|label|               words|           wordsNoSw|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|homelessness (or ...|  1.0|[homelessness, (o...|[homelessness, (,...|\n",
      "|brilliant over-ac...|  1.0|[brilliant, over-...|[brilliant, -, ac...|\n",
      "|this is easily th...|  1.0|[this, is, easily...|[easily, underrat...|\n",
      "|this is not the t...|  1.0|[this, is, not, t...|[typical, mel, br...|\n",
      "|in this \"critical...|  1.0|[in, this, \"criti...|[\", critically, a...|\n",
      "|the night listene...|  1.0|[the, night, list...|[night, listener,...|\n",
      "|you know, robin w...|  1.0|[you, know,, robi...|[know, ,, robin, ...|\n",
      "|airport '77 start...|  0.0|[airport, '77, st...|[airport, ', 77, ...|\n",
      "|this film lacked ...|  0.0|[this, film, lack...|[film, lacked, so...|\n",
      "|sorry everyone,,,...|  0.0|[sorry, everyone,...|[sorry, everyone,...|\n",
      "|the second attemp...|  0.0|[the, second, att...|[second, attempt,...|\n",
      "|i don't know who ...|  0.0|[i, don't, know, ...|[', know, blame, ...|\n",
      "|this film is medi...|  0.0|[this, film, is, ...|[film, mediocre, ...|\n",
      "|the film is bad. ...|  0.0|[the, film, is, b...|[film, bad, ., wa...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrainTokNoSw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(wordsNoSw=[u'homelessness', u'(', u'houselessness', u'george', u'carlin', u'stated', u')', u'issue', u'years', u'never', u'plan', u'help', u'street', u'considered', u'human', u'everything', u'going', u'school', u',', u'work', u',', u'vote', u'matter', u'.', u'people', u'think', u'homeless', u'lost', u'cause', u'worrying', u'things', u'racism', u',', u'war', u'iraq', u',', u'pressuring', u'kids', u'succeed', u',', u'technology', u',', u'elections', u',', u'inflation', u',', u'worrying', u\"'\", u'll', u'next', u'end', u'streets', u'.', u'given', u'bet', u'live', u'streets', u'month', u'without', u'luxuries', u'home', u',', u'entertainment', u'sets', u',', u'bathroom', u',', u'pictures', u'wall', u',', u'computer', u',', u'everything', u'treasure', u'see', u\"'\", u'like', u'homeless', u'?', u'goddard', u'bolt', u\"'\", u'lesson', u'.', u'mel', u'brooks', u'(', u'directs', u')', u'stars', u'bolt', u'plays', u'rich', u'man', u'everything', u'world', u'deciding', u'make', u'bet', u'sissy', u'rival', u'(', u'jeffery', u'tambor', u')', u'see', u'live', u'streets', u'thirty', u'days', u'without', u'luxuries', u';', u'bolt', u'succeeds', u',', u'wants', u'future', u'project', u'making', u'buildings', u'.', u'bet', u\"'\", u'bolt', u'thrown', u'street', u'bracelet', u'leg', u'monitor', u'every', u'move', u\"'\", u'step', u'sidewalk', u'.', u\"'\", u'given', u'nickname', u'pepto', u'vagrant', u\"'\", u'written', u'forehead', u'bolt', u'meets', u'characters', u'including', u'woman', u'name', u'molly', u'(', u'lesley', u'ann', u'warren', u')', u'ex', u'-', u'dancer', u'got', u'divorce', u'losing', u'home', u',', u'pals', u'sailor', u'(', u'howard', u'morris', u')', u'fumes', u'(', u'teddy', u'wilson', u')', u'already', u'used', u'streets', u'.', u\"'\", u're', u'survivors', u'.', u'bolt', u'isn', u\"'\", u'.', u\"'\", u'used', u'reaching', u'mutual', u'agreements', u'like', u'rich', u\"'\", u'fight', u'flight', u',', u'kill', u'killed', u'.', u'love', u'connection', u'molly', u'bolt', u'wasn', u\"'\", u'necessary', u'plot', u',', u'found', u'\"', u'life', u'stinks', u'\"', u'one', u'mel', u'brooks', u\"'\", u'observant', u'films', u'prior', u'comedy', u',', u'shows', u'tender', u'side', u'compared', u'slapstick', u'work', u'blazing', u'saddles', u',', u'young', u'frankenstein', u',', u'spaceballs', u'matter', u',', u'show', u\"'\", u'like', u'something', u'valuable', u'losing', u'next', u'day', u'hand', u'making', u'stupid', u'bet', u'like', u'rich', u'people', u\"'\", u'know', u'money', u'.', u'maybe', u'give', u'homeless', u'instead', u'using', u'like', u'monopoly', u'money', u'.', u'maybe', u'film', u'inspire', u'help', u'others', u'.'])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La ponctuation est bien gérée\n",
    "dfTrainTokNoSw.select('wordsNoSw').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "class NLTKPosTagger(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, tagset=None):\n",
    "        super(NLTKPosTagger, self).__init__()\n",
    "        self.tagset = Param(self, \"tagset\", \"\")\n",
    "        self._setDefault(tagset='universal')\n",
    "        kwargs = self.__init__._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, tagset=None):\n",
    "        kwargs = self.setParams._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setTagset(self, value):\n",
    "        self._paramMap[self.tagset] = value\n",
    "        return self\n",
    "\n",
    "    def getTagset(self):\n",
    "        return self.getOrDefault(self.tagset)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        tagset = self.getTagset()\n",
    "\n",
    "        def f(s):\n",
    "            return nltk.pos_tag(s,tagset=tagset)\n",
    "\n",
    "        fields = (StructField('word',StringType(),True),StructField('tag',StringType(),True))\n",
    "        t = ArrayType(StructType(fields))\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posTagger = NLTKPosTagger(\n",
    "    inputCol=\"words\", outputCol=\"tagWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTagged = posTagger.transform(dfTrainTok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|              review|label|               words|            tagWords|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|homelessness (or ...|  1.0|[homelessness, (o...|[[homelessness,NO...|\n",
      "|brilliant over-ac...|  1.0|[brilliant, over-...|[[brilliant,ADJ],...|\n",
      "|this is easily th...|  1.0|[this, is, easily...|[[this,DET], [is,...|\n",
      "|this is not the t...|  1.0|[this, is, not, t...|[[this,DET], [is,...|\n",
      "|in this \"critical...|  1.0|[in, this, \"criti...|[[in,ADP], [this,...|\n",
      "|the night listene...|  1.0|[the, night, list...|[[the,DET], [nigh...|\n",
      "|you know, robin w...|  1.0|[you, know,, robi...|[[you,PRON], [kno...|\n",
      "|airport '77 start...|  0.0|[airport, '77, st...|[[airport,NOUN], ...|\n",
      "|this film lacked ...|  0.0|[this, film, lack...|[[this,DET], [fil...|\n",
      "|sorry everyone,,,...|  0.0|[sorry, everyone,...|[[sorry,NOUN], [e...|\n",
      "|the second attemp...|  0.0|[the, second, att...|[[the,DET], [seco...|\n",
      "|i don't know who ...|  0.0|[i, don't, know, ...|[[i,PRON], [don't...|\n",
      "|this film is medi...|  0.0|[this, film, is, ...|[[this,DET], [fil...|\n",
      "|the film is bad. ...|  0.0|[the, film, is, b...|[[the,DET], [film...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTagged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "bigram = NGram(inputCol=\"words\", outputCol=\"bigrams\")\n",
    "dfBigram = bigram.transform(dfTrainTokNoSw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|              review|label|               words|           wordsNoSw|             bigrams|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|homelessness (or ...|  1.0|[homelessness, (o...|[homelessness, (,...|[homelessness (or...|\n",
      "|brilliant over-ac...|  1.0|[brilliant, over-...|[brilliant, -, ac...|[brilliant over-a...|\n",
      "|this is easily th...|  1.0|[this, is, easily...|[easily, underrat...|[this is, is easi...|\n",
      "|this is not the t...|  1.0|[this, is, not, t...|[typical, mel, br...|[this is, is not,...|\n",
      "|in this \"critical...|  1.0|[in, this, \"criti...|[\", critically, a...|[in this, this \"c...|\n",
      "|the night listene...|  1.0|[the, night, list...|[night, listener,...|[the night, night...|\n",
      "|you know, robin w...|  1.0|[you, know,, robi...|[know, ,, robin, ...|[you know,, know,...|\n",
      "|airport '77 start...|  0.0|[airport, '77, st...|[airport, ', 77, ...|[airport '77, '77...|\n",
      "|this film lacked ...|  0.0|[this, film, lack...|[film, lacked, so...|[this film, film ...|\n",
      "|sorry everyone,,,...|  0.0|[sorry, everyone,...|[sorry, everyone,...|[sorry everyone,,...|\n",
      "|the second attemp...|  0.0|[the, second, att...|[second, attempt,...|[the second, seco...|\n",
      "|i don't know who ...|  0.0|[i, don't, know, ...|[', know, blame, ...|[i don't, don't k...|\n",
      "|this film is medi...|  0.0|[this, film, is, ...|[film, mediocre, ...|[this film, film ...|\n",
      "|the film is bad. ...|  0.0|[the, film, is, b...|[film, bad, ., wa...|[the film, film i...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfBigram.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pour la suite on a le choix entre l'encodage utilisé par le prof (le mot y est ou n'y est pas)\n",
    "# ou la version en apparence plus informative du tfidf. En vrai, le tfidf peut être trompeur \n",
    "# donc je construis quand même les dictionnaires d'unigrammes et de bigrammes pour pouvoir \n",
    "# calculer les sparse vectors du prof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "lists=dfBigram.map(lambda r : r.words).collect()\n",
    "dictUnigrams=set(itertools.chain(*lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lists2=dfBigram.map(lambda r : r.bigrams).collect()\n",
    "dictBigrams=set(itertools.chain(*lists2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionaryUni={}\n",
    "for i,word in enumerate(dictUnigrams):\n",
    "\tdictionaryUni[word]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionaryBigrams={}\n",
    "for i,word in enumerate(dictBigrams):\n",
    "\tdictionaryBigrams[word]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'!' in dictionaryUni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voila qui est mieux...\n",
    "lists3=dfBigram.map(lambda r : r.wordsNoSw).collect()\n",
    "dict3=set(itertools.chain(*lists3))\n",
    "dictionary3 = {}\n",
    "for i,word in enumerate(dict3):\n",
    "    dictionary3[word]=i\n",
    "'!' in dictionary3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "r = re.compile(r'[\\s{}]+'.format(re.escape(punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";\n",
      "&\n",
      "/\n",
      ",,\n",
      "?)\n",
      ",\"\n",
      "?\"\n",
      "!\n",
      "\").\n",
      ",\n",
      "'\n",
      "\"\n",
      "\":\n",
      "*******\n",
      "-\n",
      "'.\"\n",
      "**\n",
      "(\n",
      ".\"\n",
      "))\n",
      "),\n",
      ").\n",
      ".\n",
      "~\n",
      ")\n",
      "\",\n",
      "\".\n",
      "\"?\n",
      "...\n",
      "(\"\n",
      "?\n",
      ",,,\n",
      "!)\n",
      ":\n",
      ".\")\n"
     ]
    }
   ],
   "source": [
    "for k in dict3:\n",
    "    m = r.search(k)\n",
    "    if m:\n",
    "        print m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fonction calculant le sparse vector correspondant à un ensemble de tokens\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "def vectorizeUni(tokens):\n",
    "    vector_dict={}\n",
    "    for w in tokens:\n",
    "        vector_dict[dictionaryUni[w]]=1\n",
    "    return SparseVector(len(dictionaryUni),vector_dict)\n",
    "\n",
    "def vectorizeBi(tokens):\n",
    "    vector_dict={}\n",
    "    for w in tokens:\n",
    "        vector_dict[dictionaryBigrams[w]]=1\n",
    "    return SparseVector(len(dictionaryBigrams),vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# La ca devient le bordel, j'en ai chié pour arriver à appliquer une fonction à toute une colonne \n",
    "# d'une dataframe. Contrairement à pandas, y a pas de fonction \"apply\", il faut recourir à des \n",
    "# UserDefinedFunctions, et penser que le type sparseVector ne sera pas reconnu par la dataframe, qui\n",
    "# n'est compatible qu'avec un nombre restreint de types\n",
    "\n",
    "# EDIT : en fait je m'en suis pas rendu compte, mais cette manip je l'avais déjà faite pour la surcharge des\n",
    "# tokenizer et postagger... les cinq dernières lignes à la fin avec udf et tout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "udfVectorizeUni=UserDefinedFunction(lambda x : vectorizeUni(x),\n",
    "                                    VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Une dataframe est un objet immutable, donc pas la peine d'essayer de modifier une colonne,\n",
    "# à la place on crée une deuxième dataframe où on ajoute la colonne qu'on veut.\n",
    "dfVect = dfBigram.withColumn(\"words\", udfVectorizeUni(\"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|              review|label|               words|           wordsNoSw|             bigrams|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|homelessness (or ...|  1.0|(1573,[14,26,28,3...|[homelessness, (,...|[homelessness (or...|\n",
      "|brilliant over-ac...|  1.0|(1573,[4,22,25,30...|[brilliant, -, ac...|[brilliant over-a...|\n",
      "|this is easily th...|  1.0|(1573,[37,71,85,8...|[easily, underrat...|[this is, is easi...|\n",
      "|this is not the t...|  1.0|(1573,[10,37,61,6...|[typical, mel, br...|[this is, is not,...|\n",
      "|in this \"critical...|  1.0|(1573,[0,12,20,21...|[\", critically, a...|[in this, this \"c...|\n",
      "|the night listene...|  1.0|(1573,[0,6,16,33,...|[night, listener,...|[the night, night...|\n",
      "|you know, robin w...|  1.0|(1573,[3,23,29,36...|[know, ,, robin, ...|[you know,, know,...|\n",
      "|airport '77 start...|  0.0|(1573,[7,9,11,13,...|[airport, ', 77, ...|[airport '77, '77...|\n",
      "|this film lacked ...|  0.0|(1573,[17,24,29,3...|[film, lacked, so...|[this film, film ...|\n",
      "|sorry everyone,,,...|  0.0|(1573,[8,51,96,13...|[sorry, everyone,...|[sorry everyone,,...|\n",
      "|the second attemp...|  0.0|(1573,[4,18,25,33...|[second, attempt,...|[the second, seco...|\n",
      "|i don't know who ...|  0.0|(1573,[1,5,29,64,...|[', know, blame, ...|[i don't, don't k...|\n",
      "|this film is medi...|  0.0|(1573,[104,108,11...|[film, mediocre, ...|[this film, film ...|\n",
      "|the film is bad. ...|  0.0|(1573,[0,2,29,39,...|[film, bad, ., wa...|[the film, film i...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On a bien remplacé ici du coup les mots par les vecteurs sparse\n",
    "dfVect.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "udfVectorizeBi=UserDefinedFunction(lambda x : vectorizeBi(x),\n",
    "                                   VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfVect2 = dfVect.withColumn(\"bigrams\", udfVectorizeBi(\"bigrams\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|              review|label|               words|           wordsNoSw|             bigrams|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|homelessness (or ...|  1.0|(1573,[14,26,28,3...|[homelessness, (,...|(3370,[14,16,25,3...|\n",
      "|brilliant over-ac...|  1.0|(1573,[4,22,25,30...|[brilliant, -, ac...|(3370,[7,30,33,40...|\n",
      "|this is easily th...|  1.0|(1573,[37,71,85,8...|[easily, underrat...|(3370,[6,101,124,...|\n",
      "|this is not the t...|  1.0|(1573,[10,37,61,6...|[typical, mel, br...|(3370,[4,21,84,98...|\n",
      "|in this \"critical...|  1.0|(1573,[0,12,20,21...|[\", critically, a...|(3370,[0,17,26,36...|\n",
      "|the night listene...|  1.0|(1573,[0,6,16,33,...|[night, listener,...|(3370,[9,35,38,50...|\n",
      "|you know, robin w...|  1.0|(1573,[3,23,29,36...|[know, ,, robin, ...|(3370,[1,52,54,58...|\n",
      "|airport '77 start...|  0.0|(1573,[7,9,11,13,...|[airport, ', 77, ...|(3370,[2,6,8,15,1...|\n",
      "|this film lacked ...|  0.0|(1573,[17,24,29,3...|[film, lacked, so...|(3370,[12,28,46,5...|\n",
      "|sorry everyone,,,...|  0.0|(1573,[8,51,96,13...|[sorry, everyone,...|(3370,[3,13,24,42...|\n",
      "|the second attemp...|  0.0|(1573,[4,18,25,33...|[second, attempt,...|(3370,[5,10,23,27...|\n",
      "|i don't know who ...|  0.0|(1573,[1,5,29,64,...|[', know, blame, ...|(3370,[6,31,70,83...|\n",
      "|this film is medi...|  0.0|(1573,[104,108,11...|[film, mediocre, ...|(3370,[19,106,199...|\n",
      "|the film is bad. ...|  0.0|(1573,[0,2,29,39,...|[film, bad, ., wa...|(3370,[11,36,84,1...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfVect2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               words|           normWords|\n",
      "+--------------------+--------------------+\n",
      "|(1573,[14,26,28,3...|(1573,[14,26,28,3...|\n",
      "|(1573,[4,22,25,30...|(1573,[4,22,25,30...|\n",
      "|(1573,[37,71,85,8...|(1573,[37,71,85,8...|\n",
      "|(1573,[10,37,61,6...|(1573,[10,37,61,6...|\n",
      "|(1573,[0,12,20,21...|(1573,[0,12,20,21...|\n",
      "|(1573,[0,6,16,33,...|(1573,[0,6,16,33,...|\n",
      "|(1573,[3,23,29,36...|(1573,[3,23,29,36...|\n",
      "|(1573,[7,9,11,13,...|(1573,[7,9,11,13,...|\n",
      "|(1573,[17,24,29,3...|(1573,[17,24,29,3...|\n",
      "|(1573,[8,51,96,13...|(1573,[8,51,96,13...|\n",
      "|(1573,[4,18,25,33...|(1573,[4,18,25,33...|\n",
      "|(1573,[1,5,29,64,...|(1573,[1,5,29,64,...|\n",
      "|(1573,[104,108,11...|(1573,[104,108,11...|\n",
      "|(1573,[0,2,29,39,...|(1573,[0,2,29,39,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pour les opérations de traitement du langage, il est d'usage de normaliser (L2)\n",
    "# les vecteurs de features : c'est ce qui marche le mieux apparemment.\n",
    "from pyspark.ml.feature import Normalizer\n",
    "normalizerUni = Normalizer(inputCol='words',outputCol='normWords',p=2.0)\n",
    "normalizerBi = Normalizer(inputCol=\"bigrams\",outputCol='normBigrams',p=2.0)\n",
    "dfNorm = normalizerUni.transform(dfVect2)\n",
    "dfNorm2 = normalizerBi.transform(dfNorm)\n",
    "dfNorm2.select('words','normWords').show()\n",
    "# La différence n'apparait pas dans la table puisqu'on n'a la place de visualiser que les indices des élements \n",
    "# non nuls et pas leur valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On passe au TFIDF\n",
    "# Evidemment en choisissant la bonne dataframe parmi celle du dessus, on peut appliquer ces calculs\n",
    "# à n'importz quelle colonne (bigrammes, avec stop words ou sans...)\n",
    "from pyspark.ml.feature import HashingTF\n",
    "htf = HashingTF(inputCol='words',outputCol='wordsTF',numFeatures=10000)\n",
    "dfTrainTF = htf.transform(dfTrainTokNoSw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              review|             wordsTF|          wordsTFIDF|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|homelessness (or ...|(10000,[0,14,39,7...|(10000,[0,14,39,7...|\n",
      "|brilliant over-ac...|(10000,[77,97,105...|(10000,[77,97,105...|\n",
      "|this is easily th...|(10000,[97,131,51...|(10000,[97,131,51...|\n",
      "|this is not the t...|(10000,[97,260,41...|(10000,[97,260,41...|\n",
      "|in this \"critical...|(10000,[0,45,82,9...|(10000,[0,45,82,9...|\n",
      "|the night listene...|(10000,[0,48,97,9...|(10000,[0,48,97,9...|\n",
      "|you know, robin w...|(10000,[97,105,15...|(10000,[97,105,15...|\n",
      "|airport '77 start...|(10000,[34,38,47,...|(10000,[34,38,47,...|\n",
      "|this film lacked ...|(10000,[105,124,4...|(10000,[105,124,4...|\n",
      "|sorry everyone,,,...|(10000,[49,97,102...|(10000,[49,97,102...|\n",
      "|the second attemp...|(10000,[45,97,105...|(10000,[45,97,105...|\n",
      "|i don't know who ...|(10000,[97,105,11...|(10000,[97,105,11...|\n",
      "|this film is medi...|(10000,[97,182,41...|(10000,[97,182,41...|\n",
      "|the film is bad. ...|(10000,[0,97,105,...|(10000,[0,97,105,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INverse doc frequency\n",
    "from pyspark.ml.feature import IDF\n",
    "idf = IDF(inputCol=htf.getOutputCol(),outputCol=\"wordsTFIDF\")\n",
    "idfModel = idf.fit(dfTrainTF)\n",
    "dfTrainTFIDF = idfModel.transform(dfTrainTF)\n",
    "dfTrainTFIDF.select('review','wordsTF','wordsTFIDF').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------+\n",
      "|              review|label|target_indexed|\n",
      "+--------------------+-----+--------------+\n",
      "|homelessness (or ...|  1.0|           1.0|\n",
      "|brilliant over-ac...|  1.0|           1.0|\n",
      "|this is easily th...|  1.0|           1.0|\n",
      "|this is not the t...|  1.0|           1.0|\n",
      "|in this \"critical...|  1.0|           1.0|\n",
      "|the night listene...|  1.0|           1.0|\n",
      "|you know, robin w...|  1.0|           1.0|\n",
      "|airport '77 start...|  0.0|           0.0|\n",
      "|this film lacked ...|  0.0|           0.0|\n",
      "|sorry everyone,,,...|  0.0|           0.0|\n",
      "|the second attemp...|  0.0|           0.0|\n",
      "|i don't know who ...|  0.0|           0.0|\n",
      "|this film is medi...|  0.0|           0.0|\n",
      "|the film is bad. ...|  0.0|           0.0|\n",
      "+--------------------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Je sais que cette étape m'a été utile une fois, la ça a pas trop l'air\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "string_indexer = StringIndexer(inputCol='label', outputCol='target_indexed')\n",
    "string_indexer_model = string_indexer.fit(dfTrainTFIDF)\n",
    "dfTrainFinal = string_indexer_model.transform(dfTrainTFIDF)\n",
    "dfTrainFinal.select('review','label','target_indexed').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol=idf.getOutputCol(),labelCol=string_indexer.getOutputCol())\n",
    "dt_model = dt.fit(dfTrainFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On applique le même à notre ensemble de test ridicule.\n",
    "# En théorie le pipeline permet d'automatiser tout ça mais bon, on s'en servira probablement pas\n",
    "\n",
    "# EDIT : en fait c'est plutot facile de créer des transformers à partir de chaque étape, donc peut \n",
    "# être que les pipelines c'est faisables. A voir\n",
    "df_test_words = tokenizer.transform(dfTest)\n",
    "df_test_tf = htf.transform(df_test_words)\n",
    "df_test_tfidf = idfModel.transform(df_test_tf)\n",
    "df_test_final = string_indexer_model.transform(df_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Les prédictions\n",
    "df_test_pred = dt_model.transform(df_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----------+-----------+\n",
      "|              review|target_indexed|prediction|probability|\n",
      "+--------------------+--------------+----------+-----------+\n",
      "|bromwell high is ...|           1.0|       0.0|  [1.0,0.0]|\n",
      "|this isn't the co...|           1.0|       1.0|  [0.0,1.0]|\n",
      "|yes its an art......|           1.0|       1.0|  [0.0,1.0]|\n",
      "|story of a man wh...|           0.0|       0.0|  [1.0,0.0]|\n",
      "|when i was little...|           0.0|       1.0|  [0.0,1.0]|\n",
      "+--------------------+--------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_pred.select('review', 'target_indexed', 'prediction', 'probability').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Je fais un pipeline très basique\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instanciate all the Estimators and Transformers necessary\n",
    "tokenizer = Tokenizer(inputCol='review', outputCol='reviews_words')\n",
    "hashing_tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='reviews_tf', numFeatures=10000)\n",
    "idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"reviews_tfidf\")\n",
    "string_indexer = StringIndexer(inputCol='label', outputCol='target_indexed')\n",
    "dt = DecisionTreeClassifier(featuresCol=idf.getOutputCol(), labelCol=string_indexer.getOutputCol(), maxDepth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instanciate a Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                            hashing_tf,\n",
    "                            idf,\n",
    "                            string_indexer,\n",
    "                            dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(dfTrain)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_pred = pipeline_model.transform(dfTest)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----------+-----------+\n",
      "|              review|target_indexed|prediction|probability|\n",
      "+--------------------+--------------+----------+-----------+\n",
      "|bromwell high is ...|           1.0|       0.0|  [1.0,0.0]|\n",
      "|this isn't the co...|           1.0|       1.0|  [0.0,1.0]|\n",
      "|yes its an art......|           1.0|       1.0|  [0.0,1.0]|\n",
      "|story of a man wh...|           0.0|       0.0|  [1.0,0.0]|\n",
      "|when i was little...|           0.0|       1.0|  [0.0,1.0]|\n",
      "|\"it appears that ...|           0.0|       1.0|  [0.0,1.0]|\n",
      "+--------------------+--------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_pred.select('review', 'target_indexed', 'prediction', 'probability').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Un outil automatique pour calculer le taux de bonne classif.\n",
    "# La encore pas très utile en vrai\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='target_indexed', metricName='precision')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(df_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# La cross validation et le test des différents paramètres du classifieurs c'est pas trop dur sur spark ML, \n",
    "# c'est en fait la seule raison pour laquelle cette librairie me paraissait mieux... avec du recul j'aurais \n",
    "# perdu moins de temps à recoder moi-même la cross valid et la grid search..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid=(ParamGridBuilder()\n",
    "     .baseOn([evaluator.metricName,'precision'])\n",
    "     .addGrid(dt.maxDepth, [10,20])\n",
    "     .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=grid,evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_model = cv.fit(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test_pred = cv_model.transform(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(df_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
